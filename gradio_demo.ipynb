{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install Gradio (using -q for quiet mode)\n",
        "!pip install gradio -q\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "import gradio as gr\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "print(\"Environment setup complete, Google Drive mounted.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdklC_L_Ibpc",
        "outputId": "5d79fcc0-4ba2-4897-b7fe-908536870798"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Environment setup complete, Google Drive mounted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration and Data Loading ---\n",
        "# Please modify this path according to your Google Drive\n",
        "dataset_path = '/content/drive/MyDrive/HAR prepocessed dataset/COEN498-691_HAR_preprocessed_dataset.csv'\n",
        "\n",
        "if not os.path.exists(dataset_path):\n",
        "    raise FileNotFoundError(f\"Dataset file not found, please check the path: {dataset_path}\")\n",
        "\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "# 1. Prepare Data (consistent with original notebook)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df.drop(['activity_id', 'participant_id'], axis=1)\n",
        "y = df['activity_id']\n",
        "\n",
        "# Split data (for training Scaler and Encoder)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# 2. Preprocessing\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y_train_encoded = encoder.fit_transform(y_train.values.reshape(-1, 1))\n",
        "\n",
        "# 3. Model Definition (consistent with original notebook architecture)\n",
        "n_features = X_train_scaled.shape[1]\n",
        "n_classes = y_train_encoded.shape[1]\n",
        "\n",
        "model = Sequential()\n",
        "# UserWarning: Do not pass an `input_shape`/`input_dim`...\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    model.add(Dense(128, activation='relu', input_shape=(n_features,)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(n_classes, activation='softmax'))\n",
        "\n",
        "# TODO load pretrained model weights file\n",
        "weights_path = '' # <--- Please adjust this path to your actual weights file (e.g., '/content/drive/MyDrive/my_model_weights.h5')\n",
        "if os.path.exists(weights_path):\n",
        "    try:\n",
        "        model.load_weights(weights_path)\n",
        "        print(f\"Loaded pretrained model weights from {weights_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading weights from {weights_path}: {e}. Model will be trained from scratch.\")\n",
        "else:\n",
        "    print(f\"Pretrained weights file not found at {weights_path}. Model will be trained from scratch.\")\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 4. Model Training (reduced epochs for quick demonstration)\n",
        "print(\"Starting MLP model training (Epochs=10)...\")\n",
        "# Note: The original notebook used epochs=50 for high accuracy,\n",
        "# we use 10 here for faster demonstration.\n",
        "model.fit(X_train_scaled, y_train_encoded, epochs=10, batch_size=32, validation_split=0.2, verbose=0)\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# 5. Define Gradio Prediction Function and Label Mapping\n",
        "activity_ids_order = encoder.categories_[0].tolist()\n",
        "ACTIVITY_NAMES = {\n",
        "    1: \"sitting\",\n",
        "    2: \"walking\",\n",
        "    3: \"running\",\n",
        "    4: \"lying\"\n",
        "}\n",
        "LABEL_MAP = {i: ACTIVITY_NAMES.get(activity_ids_order[i], f\"Activity ID {activity_ids_order[i]}\")\n",
        "             for i in range(len(activity_ids_order))}\n",
        "\n",
        "\n",
        "def predict_activity(*features):\n",
        "    \"\"\"Gradio wrapper function for prediction.\"\"\"\n",
        "    try:\n",
        "        # Convert input and scale (using scaler fitted during training)\n",
        "        input_data = np.array(features, dtype=np.float32).reshape(1, -1)\n",
        "        scaled_data = scaler.transform(input_data)\n",
        "\n",
        "        # Predict\n",
        "        predictions = model.predict(scaled_data, verbose=0)[0]\n",
        "\n",
        "        # Decode and format output\n",
        "        predicted_index = np.argmax(predictions)\n",
        "        confidence = predictions[predicted_index]\n",
        "        predicted_label = LABEL_MAP.get(predicted_index, f\"Unknown Index {predicted_index}\")\n",
        "\n",
        "        output_text = (\n",
        "            f\"Predicted Activity: **{predicted_label}**\\n\"\n",
        "            f\"Confidence: {confidence:.4f}\\n\\n\"\n",
        "            \"--- Raw Probability Distribution (Softmax) ---\\n\"\n",
        "        )\n",
        "\n",
        "        sorted_indices = np.argsort(predictions)[::-1]\n",
        "        for i in sorted_indices:\n",
        "            label_name = LABEL_MAP.get(i, f\"Unknown Index {i}\")\n",
        "            output_text += f\"{label_name}: {predictions[i]:.4f}\\n\"\n",
        "\n",
        "        return output_text\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error during prediction: {e}\"\n",
        "\n",
        "# 6. Configure Gradio Interface\n",
        "feature_names = X.columns.tolist()\n",
        "feature_inputs = [gr.Number(label=name, value=0.0) for name in feature_names]\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=predict_activity,\n",
        "    inputs=feature_inputs,\n",
        "    outputs=gr.Markdown(label=\"Prediction Result\"),\n",
        "    title=\"Human Activity Recognition (HAR) Predictor\",\n",
        "    description=\"Enter 37 sensor feature values (**these should be standardized feature values**), and the model will predict the activity type.\",\n",
        "    live=False\n",
        ")\n",
        "\n",
        "# 7. Launch Gradio Application\n",
        "print(\"\\n--- Gradio Interface Launched ---\")\n",
        "iface.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "id": "FOErma-1Iiaz",
        "outputId": "04bb8f62-f559-4eb6-c0a3-1350059a2882"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretrained weights file not found at . Model will be trained from scratch.\n",
            "Starting MLP model training (Epochs=10)...\n",
            "Model training complete.\n",
            "\n",
            "--- Gradio Interface Launched ---\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://2260cf7af9f8e0e1c6.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://2260cf7af9f8e0e1c6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0Tjmh8Jj5Fk0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}