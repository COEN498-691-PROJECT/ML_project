{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34d34698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e23dfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_ids = [\"LL\", \"ZM\", \"VV\", \"YL\", \"YT\"] \n",
    "activity_ids = [\"sitting\", \"walking\", \"running\", \"lying\"]\n",
    "sensor_ids = [\"ax\", \"ay\", \"az\"]\n",
    "columns_order = ['participant_id', 'activity_id', 'timestamp', 'ax', 'ay', 'az']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acd4600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_list():\n",
    "    \"\"\"\n",
    "    Fetch the list of files from the GitHub repository\n",
    "    \"\"\"\n",
    "    url = f\"https://api.github.com/repos/COEN498-691-PROJECT/ML_project/contents/data/raw?ref=main\"\n",
    "    response = requests.get(url)\n",
    "    files = response.json()\n",
    "    return files\n",
    "\n",
    "def filter_files(files, participant_ids, activity_ids, sensor_ids):\n",
    "    \"\"\"\n",
    "    Filter the list of files based on the list of participant IDs, activity IDs, and sensor IDs\n",
    "    \"\"\"\n",
    "    filtered_files_url = []\n",
    "    for file in files:\n",
    "        file_name = file['name']\n",
    "        if any(pid.upper() in file_name for pid in participant_ids) and \\\n",
    "           any(aid in file_name for aid in activity_ids) and \\\n",
    "           any(sid.upper() in file_name for sid in sensor_ids):\n",
    "            filtered_files_url.append( (file['download_url'], file_name) )\n",
    "    return filtered_files_url\n",
    "\n",
    "def add_columns(df, file_url):\n",
    "    \"\"\"\n",
    "    Add participant ID and activity ID to the dataframe\n",
    "    \"\"\"\n",
    "    parts = file_url.split('_')\n",
    "    participant_id = parts[0]\n",
    "    activity_id = parts[1]\n",
    "    df['participant_id'] = participant_id\n",
    "    df['activity_id'] = activity_id\n",
    "    return df\n",
    "\n",
    "def load_dataframes(file_urls: tuple):\n",
    "    \"\"\"\n",
    "    Load dataframes from the list of file URLs and add participant and activity IDs columns\n",
    "    \"\"\"\n",
    "    df_list = []\n",
    "    for file_url, file_name in file_urls:\n",
    "        df = pd.read_csv(file_url)\n",
    "        df = add_columns(df, file_name)\n",
    "        df_list.append(df)\n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558083d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_timestamps(df_list):\n",
    "    \"\"\"\n",
    "    Merge axes per participant-activity on a single timeline by using interpolation\n",
    "    \"\"\"\n",
    "    # Step 1: identify all participant-activity combinations\n",
    "    combos = {(df['participant_id'].iloc[0], df['activity_id'].iloc[0]) for df in df_list}\n",
    "    \n",
    "    merged_dfs = []\n",
    "\n",
    "    for pid, aid in combos:\n",
    "        # Select AX, AY, AZ for this participant-activity\n",
    "        dfs = [df for df in df_list if df['participant_id'].iloc[0] == pid and df['activity_id'].iloc[0] == aid]\n",
    "        \n",
    "        ax_df = next(df for df in dfs if 'AX' in df.columns)\n",
    "        ay_df = next(df for df in dfs if 'AY' in df.columns)\n",
    "        az_df = next(df for df in dfs if 'AZ' in df.columns)\n",
    "\n",
    "        # Sort by timestamp\n",
    "        ax_df = ax_df.sort_values('LocalTimestamp').set_index('LocalTimestamp')\n",
    "        ay_df = ay_df.sort_values('LocalTimestamp').set_index('LocalTimestamp')\n",
    "        az_df = az_df.sort_values('LocalTimestamp').set_index('LocalTimestamp')\n",
    "\n",
    "        # Interpolate AY and AZ to AX timestamps\n",
    "        df_acc = pd.DataFrame(index=ax_df.index)\n",
    "        df_acc['ax'] = ax_df['AX'].values\n",
    "        df_acc['ay'] = np.interp(ax_df.index.values, ay_df.index.values, ay_df['AY'].values)\n",
    "        df_acc['az'] = np.interp(ax_df.index.values, az_df.index.values, az_df['AZ'].values)\n",
    "\n",
    "        # Add participant_id and activity_id\n",
    "        df_acc['participant_id'] = pid\n",
    "        df_acc['activity_id'] = aid\n",
    "\n",
    "        # Reset index and rename timestamp\n",
    "        df_acc.reset_index(inplace=True)\n",
    "        df_acc.rename(columns={'index':'timestamp'}, inplace=True)\n",
    "\n",
    "        # Only append if timestamp column exists\n",
    "        if 'timestamp' in df_acc.columns:\n",
    "            # Safe reorder (skip missing columns)\n",
    "            df_acc = df_acc[[col for col in columns_order if col in df_acc.columns]]\n",
    "            merged_dfs.append(df_acc)\n",
    "        else:\n",
    "            print(f\"Warning: participant {pid}, activity {aid} skipped because 'timestamp' missing\")\n",
    "\n",
    "    # Combine all\n",
    "    combined_df = pd.concat(merged_dfs, ignore_index=True)\n",
    "\n",
    "    # Sort by timestamp\n",
    "    combined_df.sort_values(by=['timestamp'], inplace=True)\n",
    "\n",
    "    # Save CSV\n",
    "    combined_df.to_csv('../../data/processed/COEN498-691_HAR_dataset.csv',\n",
    "                       index=False, date_format='%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "    return combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20789ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_file(df_list):\n",
    "    \"\"\"\n",
    "    Create the final dataset file by combining all dataframes, cleaning, and saving to CSV\n",
    "    \"\"\"\n",
    "    combined_df = pd.concat(df_list, ignore_index=True) # Combine all dataframes\n",
    "    combined_df.drop(columns=['PacketNumber', 'DataLength', 'TypeTag', 'ProtocolVersion', 'EmotiBitTimestamp', 'DataReliability'], errors='ignore', inplace=True) # Drop unnecessary columns\n",
    "    combined_df.rename(columns={'LocalTimestamp': 'timestamp',\n",
    "                                'AX': 'ax',\n",
    "                                'AY': 'ay',\n",
    "                                'AZ': 'az',}, inplace=True) # Rename columns for consistency\n",
    "    combined_df = combined_df[columns_order] # Reorder columns\n",
    "    combined_df = combined_df.groupby(['timestamp', 'activity_id', 'participant_id']).mean().reset_index() # Handle duplicates by averaging\n",
    "    combined_df.sort_values(by=['timestamp'], inplace=True) # Sort by timestamp (ascending)\n",
    "    combined_df.to_csv('../../data/processed/COEN498-691_HAR_dataset.csv', index=False, date_format='%Y-%m-%d %H:%M:%S.%f') # Save to CSV\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cb2f506",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = get_file_list()\n",
    "filtered_files_url = filter_files(file_list, participant_ids, activity_ids, sensor_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9c3a155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   LocalTimestamp  EmotiBitTimestamp  PacketNumber  DataLength TypeTag  \\\n",
      "0    1.760382e+09          1325026.0         21641           1      AX   \n",
      "1    1.760382e+09          1325066.0         21657           2      AX   \n",
      "2    1.760382e+09          1325106.0         21657           2      AX   \n",
      "3    1.760382e+09          1325146.0         21675           3      AX   \n",
      "4    1.760382e+09          1325186.0         21675           3      AX   \n",
      "5    1.760382e+09          1325226.0         21675           3      AX   \n",
      "6    1.760382e+09          1325266.0         21691           2      AX   \n",
      "7    1.760382e+09          1325306.0         21691           2      AX   \n",
      "8    1.760382e+09          1325346.0         21708           3      AX   \n",
      "9    1.760382e+09          1325386.0         21708           3      AX   \n",
      "\n",
      "   ProtocolVersion  DataReliability     AX participant_id activity_id  \n",
      "0                1              100  0.596             LL       lying  \n",
      "1                1              100  0.710             LL       lying  \n",
      "2                1              100  0.670             LL       lying  \n",
      "3                1              100  0.664             LL       lying  \n",
      "4                1              100  0.688             LL       lying  \n",
      "5                1              100  0.727             LL       lying  \n",
      "6                1              100  0.703             LL       lying  \n",
      "7                1              100  0.703             LL       lying  \n",
      "8                1              100  0.725             LL       lying  \n",
      "9                1              100  0.677             LL       lying  \n"
     ]
    }
   ],
   "source": [
    "df_list = load_dataframes(filtered_files_url)\n",
    "print(df_list[0].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6152ebb5",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'timestamp'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_8312\\2067276265.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df_final = align_timestamps(df_list)\n\u001b[32m      2\u001b[39m print(df_final.head(\u001b[32m10\u001b[39m))\n",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_8312\\2530237862.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(df_list)\u001b[39m\n\u001b[32m     42\u001b[39m     \u001b[38;5;66;03m# Combine all\u001b[39;00m\n\u001b[32m     43\u001b[39m     combined_df = pd.concat(merged_dfs, ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     44\u001b[39m \n\u001b[32m     45\u001b[39m     \u001b[38;5;66;03m# Sort by timestamp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     combined_df.sort_values(by=[\u001b[33m'timestamp'\u001b[39m], inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     47\u001b[39m \n\u001b[32m     48\u001b[39m     \u001b[38;5;66;03m# Save CSV\u001b[39;00m\n\u001b[32m     49\u001b[39m     combined_df.to_csv('../../data/processed/COEN498-691_HAR_dataset.csv',\n",
      "\u001b[32mc:\\Users\\laber\\Desktop\\COEN691\\Project\\ML_project\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[39m\n\u001b[32m   7207\u001b[39m             )\n\u001b[32m   7208\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m len(by):\n\u001b[32m   7209\u001b[39m             \u001b[38;5;66;03m# len(by) == 1\u001b[39;00m\n\u001b[32m   7210\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m7211\u001b[39m             k = self._get_label_or_level_values(by[\u001b[32m0\u001b[39m], axis=axis)\n\u001b[32m   7212\u001b[39m \n\u001b[32m   7213\u001b[39m             \u001b[38;5;66;03m# need to rewrap column in Series to apply key function\u001b[39;00m\n\u001b[32m   7214\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32mc:\\Users\\laber\\Desktop\\COEN691\\Project\\ML_project\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1910\u001b[39m             values = self.xs(key, axis=other_axes[\u001b[32m0\u001b[39m])._values\n\u001b[32m   1911\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self._is_level_reference(key, axis=axis):\n\u001b[32m   1912\u001b[39m             values = self.axes[axis].get_level_values(key)._values\n\u001b[32m   1913\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1914\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(key)\n\u001b[32m   1915\u001b[39m \n\u001b[32m   1916\u001b[39m         \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[32m   1917\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m values.ndim > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'timestamp'"
     ]
    }
   ],
   "source": [
    "df_final = align_timestamps(df_list)\n",
    "print(df_final.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
